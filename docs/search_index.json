[
["index.html", "fulltext manual Chapter 1 fulltext manual 1.1 Info 1.2 Citing fulltext 1.3 Installation", " fulltext manual built on 2018-01-16 - for fulltext v1.0.0 Chapter 1 fulltext manual An R package to search across and get full text for journal articles The fulltext package makes it easy to do text-mining by supporting the following steps: Search for articles Fetch articles Get links for full text articles (xml, pdf) Extract text from articles / convert formats Collect bits of articles that you actually need Download supplementary materials from papers 1.1 Info Code: https://github.com/ropensci/fulltext/ Issues/Bug reports: https://github.com/ropensci/fulltext/issues CRAN: https://cran.rstudio.com/web/packages/fulltext/ 1.2 Citing fulltext Scott Chamberlain &amp; Will Pearse (2018). fulltext: Full Text of ‘Scholarly’ Articles Across Many Data Sources. R package version 1.0.0. https://github.com/ropensci/fulltext 1.3 Installation Stable version from CRAN install.packages(&quot;fulltext&quot;) Development version from GitHub devtools::install_github(&quot;ropensci/fulltext&quot;) Load library library(&#39;fulltext&#39;) "],
["intro.html", "Chapter 2 Introduction 2.1 User interface", " Chapter 2 Introduction 2.1 User interface Functions in fulltext are setup to make the package as easy to use as possible. The functions are organized around use cases: Search for articles Get full text links Get articles Get abstracts Pull out article sections of interest Because there are so many data sources for scholarly texts, it makes a lot of sense to simplify the details of each data source, and present a single user interface to all of them. "],
["data-sources.html", "Chapter 3 Data sources 3.1 Search 3.2 Abstracts 3.3 Links 3.4 Getting full text", " Chapter 3 Data sources Data sources in fulltext include: Crossref - via the rcrossref package Public Library of Science (PLOS) - via the rplos package Biomed Central arXiv - via the aRxiv package bioRxiv - via the biorxivr package PMC/Pubmed via Entrez - via the rentrez package Many more are supported via the above sources (e.g., Royal Society Open Science is available via Pubmed) We will add more, as publishers open up, and as we have time…See the master list here Data sources will differ by the task you are doing in fulltext. 3.1 Search When searching with ft_search() you’ll have access to a specific set of sources and no others, including: arxiv biorxivr bmc crossref entrez europe_pmc ma plos scopus You can see what plugins there are with ft_search_ls() 3.2 Abstracts When using ft_abstract() you have access to: crossref microsoft plos scopus You can see what plugins there are with ft_abstract_ls() 3.3 Links When using ft_links() to get links to full text, you’ll have access to: bmc cogent copernicus crossref elife entrez frontiersin peerj plos You can see what plugins there are with ft_links_ls() 3.4 Getting full text While using ft_get() to fetch full text of articles you’ll have access to a set of specific data sources (in this case publishers) for which we have some coded plugins (i.e., functions): aaas arxiv biorxiv bmc copernicus crossref elife elsevier entrez frontiersin ieee informa peerj pensoft plos pnas royalsocchem scientificsocieties wiley You can see what plugins there are with ft_get_ls() But there are also other options within ft_get() that we take advantage of. This is because DOIs (Digital Object Identifiers) which you feed into ft_get() have a prefix that is affiliated with a specific publisher. We can then decide whether to use one of our plugins listed in ft_get_ls() or something else. If we don’t have a plugin we first look to see if Crossref has the full text link to either XML or PDF for the DOI. If not, we then go to an API rOpenSci maintains at https://ftdoi.org1. This API has a set of rules for each publisher - some of which are simple rules like add a URL plus a DOI - but some require an HTTP request then some string manipulation. "],
["authentication.html", "Chapter 4 Authentication", " Chapter 4 Authentication Some data sources require authentication. Here’s a breakdown of how to do authentication by data source: BMC: BMC is integrated into Springer Publishers now, and that API requires an API key. Get your key by signing up at https://dev.springer.com/, then you’ll get a key. Pass the key to a named parameter key to bmcopts. Or, save your key in your .Renviron file as SPRINGER_KEY, and we’ll read it in for you, and you don’t have to pass in anything. Scopus: Scopus requires an API key to search their service. Go to https://dev.elsevier.com/index.html, register for an account, then when you’re in your account, create an API key. Pass in as variable key to scopusopts, or store your key under the name ELSEVIER_SCOPUS_KEY as an environment variable in .Renviron, and we’ll read it in for you. See ?Startup in R for help. Microsoft: Get a key by creating an Azure account at https://www.microsoft.com/cognitive-services/en-us/subscriptions, then requesting a key for Academic Knowledge API within Cognitive Services. Store it as an environment variable in your .Renviron file - see [Startup] for help. Pass your API key into maopts as a named element in a list like list(key = Sys.getenv('MICROSOFT_ACADEMIC_KEY')) Crossref: Crossref encourages requests with contact information (an email address) and will forward you to a dedicated API cluster for improved performance when you share your email address with them. https://github.com/CrossRef/rest-api-doc#good-manners--more-reliable-service To pass your email address to Crossref via this client, store it as an environment variable in .Renviron like crossref_email = name@example.com None needed for PLOS, eLife, arxiv, biorxiv, Euro PMC, or Entrez (though soon you will get better rate limtits with auth for Entrez) "],
["search.html", "Chapter 5 Search 5.1 Usage", " Chapter 5 Search Search is what you’ll likely start with for a number of reasons. First, search functionality in fulltext means that you can start from searching on words like ‘ecology’ or ‘cellular’ - and the output of that search can be fed downstream to the next major task: fetching articles. 5.1 Usage library(fulltext) List backends available ft_search_ls() #&gt; [1] &quot;arxiv&quot; &quot;biorxivr&quot; &quot;bmc&quot; &quot;crossref&quot; &quot;entrez&quot; #&gt; [6] &quot;europe_pmc&quot; &quot;ma&quot; &quot;plos&quot; &quot;scopus&quot; Search - by default searches against PLOS (Public Library of Science) res &lt;- ft_search(query = &quot;ecology&quot;) The output of ft_search is a ft S3 object, with a summary of the results: res #&gt; Query: #&gt; [ecology] #&gt; Found: #&gt; [PLoS: 41464; BMC: 0; Crossref: 0; Entrez: 0; arxiv: 0; biorxiv: 0; Europe PMC: 0; Scopus: 0; Microsoft: 0] #&gt; Returned: #&gt; [PLoS: 10; BMC: 0; Crossref: 0; Entrez: 0; arxiv: 0; biorxiv: 0; Europe PMC: 0; Scopus: 0; Microsoft: 0] and has slots for each data source: names(res) #&gt; [1] &quot;plos&quot; &quot;bmc&quot; &quot;crossref&quot; &quot;entrez&quot; &quot;arxiv&quot; &quot;biorxiv&quot; #&gt; [7] &quot;europmc&quot; &quot;scopus&quot; &quot;ma&quot; Get data for a single source res$plos #&gt; Query: [ecology] #&gt; Records found, returned: [41464, 10] #&gt; License: [CC-BY] #&gt; id #&gt; 1 10.1371/journal.pone.0001248 #&gt; 2 10.1371/journal.pone.0059813 #&gt; 3 10.1371/journal.pone.0155019 #&gt; 4 10.1371/journal.pone.0080763 #&gt; 5 10.1371/journal.pone.0150648 #&gt; 6 10.1371/journal.pcbi.1003594 #&gt; 7 10.1371/journal.pone.0102437 #&gt; 8 10.1371/journal.pone.0175014 #&gt; 9 10.1371/journal.pone.0166559 #&gt; 10 10.1371/journal.pone.0054689 "],
["abstract.html", "Chapter 6 Abstract 6.1 Usage", " Chapter 6 Abstract Abstracts likely will come after searching for articles with ft_search(). There are a few scenarios in which simply getting abstracts in lieu of full text may be enough. For example, if you know that a large portion of the articles you want to mine text from are closed access and you don’t have access to them, you may have access to the abstracts depending on the publisher. In addition, there are cases in which you really only need abstracts regardless of whether full text is available or not. ft_abstract() gives you access to the following data sources: crossref microsoft plos scopus 6.1 Usage library(fulltext) List data sources available ft_abstract_ls() #&gt; [1] &quot;crossref&quot; &quot;microsoft&quot; &quot;plos&quot; &quot;scopus&quot; Search - by default searches against PLOS (Public Library of Science) res &lt;- ft_search(query = &quot;ecology&quot;) (dois &lt;- res$plos$data$id) #&gt; [1] &quot;10.1371/journal.pone.0001248&quot; &quot;10.1371/journal.pone.0059813&quot; #&gt; [3] &quot;10.1371/journal.pone.0155019&quot; &quot;10.1371/journal.pone.0080763&quot; #&gt; [5] &quot;10.1371/journal.pone.0150648&quot; &quot;10.1371/journal.pcbi.1003594&quot; #&gt; [7] &quot;10.1371/journal.pone.0102437&quot; &quot;10.1371/journal.pone.0175014&quot; #&gt; [9] &quot;10.1371/journal.pone.0166559&quot; &quot;10.1371/journal.pone.0054689&quot; Take the output of ft_search() and pass to ft_abstract(): out &lt;- ft_abstract(dois) and has slots for each data source: names(out) #&gt; [1] &quot;plos&quot; &quot;scopus&quot; &quot;ma&quot; &quot;crossref&quot; "],
["links.html", "Chapter 7 Links 7.1 Usage", " Chapter 7 Links The ft_links function makes it easy to get URLs for full text versions of articles. You can for instance only use fulltext to pass DOIs directly to ft_links to get URLs to use elsewhere in your research workflow. Or you may want to search first with ft_search, then pass that output directly to ft_links. 7.1 Usage library(fulltext) List backends available ft_links_ls() #&gt; [1] &quot;bmc&quot; &quot;cogent&quot; &quot;copernicus&quot; &quot;crossref&quot; &quot;elife&quot; #&gt; [6] &quot;entrez&quot; &quot;frontiersin&quot; &quot;peerj&quot; &quot;plos&quot; You can pass DOIs directly to ft_links (res &lt;- ft_links(&#39;10.3389/fphar.2014.00109&#39;)) #&gt; &lt;fulltext links&gt; #&gt; [Found] 1 #&gt; [IDs] 10.3389/fphar.2014.00109 ... res$frontiersin #&gt; $found #&gt; [1] 1 #&gt; #&gt; $ids #&gt; [1] &quot;10.3389/fphar.2014.00109&quot; #&gt; #&gt; $data #&gt; $data$`10.3389/fphar.2014.00109` #&gt; $data$`10.3389/fphar.2014.00109`$xml #&gt; [1] &quot;http://journal.frontiersin.org/article/10.3389/fphar.2014.00109/xml/nlm&quot; #&gt; #&gt; $data$`10.3389/fphar.2014.00109`$pdf #&gt; [1] &quot;http://journal.frontiersin.org/article/10.3389/fphar.2014.00109/pdf&quot; #&gt; #&gt; #&gt; #&gt; $opts #&gt; list() Or search first (res1 &lt;- ft_search(query=&#39;ecology&#39;, from=&#39;entrez&#39;)) #&gt; Query: #&gt; [ecology] #&gt; Found: #&gt; [PLoS: 0; BMC: 0; Crossref: 0; Entrez: 140275; arxiv: 0; biorxiv: 0; Europe PMC: 0; Scopus: 0; Microsoft: 0] #&gt; Returned: #&gt; [PLoS: 0; BMC: 0; Crossref: 0; Entrez: 10; arxiv: 0; biorxiv: 0; Europe PMC: 0; Scopus: 0; Microsoft: 0] Then pass the output of that directly to ft_links (out &lt;- ft_links(res1)) #&gt; &lt;fulltext links&gt; #&gt; [Found] 10 #&gt; [IDs] ID_29333349 ID_29333344 ID_29321529 ID_29321528 ID_29321519 #&gt; ID_29321473 ID_29319501 ID_29317655 ID_29317639 ID_29317616 ... "],
["fetch.html", "Chapter 8 Fetch 8.1 Data formats 8.2 How data is stored 8.3 Usage 8.4 Errors 8.5 Cleanup 8.6 Internals 8.7 Notes about specific data sources", " Chapter 8 Fetch The ft_get function makes it easy to fetch full text articles. There are a few different ways to use ft_get: Pass in only DOIs - leave from parameter NULL. This route will first query Crossref API for the publisher of the DOI, then we’ll use the appropriate method to fetch full text from the publisher. If a publisher is not found for the DOI, then we’ll throw back a message telling you a publisher was not found. Pass in DOIs (or other pub IDs) and use the from parameter. This route means we don’t have to make an extra API call to Crossref (thus, this route is faster) to determine the publisher for each DOI. We go straight to getting full text based on the publisher. Use ft_search() to search for articles. Then pass that output to this function, which will use info in that object. This behaves the same as the previous option in that each DOI has publisher info so we know how to get full text for each DOI. Note that some publishers are available through other data sources, e.g., through Entrez’s Pubmed. ft_get is a bit complicated. These are just some of the hurdles we’re jumping over: Negotiating various user inputs, likely seeing new publishers we’ve not dealt with Dealing with authentication and trying to make it easier for users Users sometimes being at an IP address that has access to a publisher and sometimes not Caching results to avoid unnecessary downloads if the content has already been acquired Thus, expect some hiccups here, and please do report problems, and if a certain publisher is not supported yet. 8.1 Data formats You can specify whether you want PDF, XML or plaint text with the type parameter. It is sometimes ignored, sometimes used, depending on the data source. For certain data sources, they only accept one type. Details by data source/publisher: PLOS: pdf and xml Entrez: only xml eLife: pdf and xml Pensoft: pdf and xml arXiv: only pdf BiorXiv: only pdf Elsevier: pdf and plain Wiley: only pdf Peerj: pdf and xml Informa: only pdf FrontiersIn: pdf and xml Copernicus: pdf and xml Scientific Societies: only pdf Crossref: depends on the publisher other data sources/publishers: there are too many to cover here - will try to make a helper in the future for what is covered by different publishers 8.2 How data is stored This depends on what backend value you use. If you use the default (rds) we store all data in .rds files. These are binary compressed files that are specific to R. Because they are specific to R, you don’t want to use this option if part of your downstream workflow is using another tool/programming language. The three types are stored in differnt ways. xml and plain text are parsed to plain text then stored with whatever backend you choose. However, pdf is retrived as raw bytes and stored as such. Thus, we no longer write pdf files to disk. However, you can easily do that yourself with ft_extract() or yourself by using pdftools::pdf_text which accepts a file path to a pdf or raw bytes. 8.3 Usage library(fulltext) List backends available ft_get_ls() #&gt; [1] &quot;aaas&quot; &quot;arxiv&quot; &quot;biorxiv&quot; #&gt; [4] &quot;bmc&quot; &quot;copernicus&quot; &quot;crossref&quot; #&gt; [7] &quot;elife&quot; &quot;elsevier&quot; &quot;entrez&quot; #&gt; [10] &quot;frontiersin&quot; &quot;ieee&quot; &quot;informa&quot; #&gt; [13] &quot;peerj&quot; &quot;pensoft&quot; &quot;plos&quot; #&gt; [16] &quot;pnas&quot; &quot;royalsocchem&quot; &quot;scientificsocieties&quot; #&gt; [19] &quot;wiley&quot; The simplest approach is passing a DOI directly to ft_get (res &lt;- ft_get(&#39;10.1371/journal.pone.0086169&#39;)) #&gt; &lt;fulltext text&gt; #&gt; [Docs] 1 #&gt; [Source] ext - /Users/sckott/Library/Caches/R/fulltext #&gt; [IDs] 10.1371/journal.pone.0086169 ... res$plos #&gt; $found #&gt; [1] 1 #&gt; #&gt; $dois #&gt; [1] &quot;10.1371/journal.pone.0086169&quot; #&gt; #&gt; $data #&gt; $data$backend #&gt; [1] &quot;ext&quot; #&gt; #&gt; $data$cache_path #&gt; [1] &quot;/Users/sckott/Library/Caches/R/fulltext&quot; #&gt; #&gt; $data$path #&gt; $data$path$`10.1371/journal.pone.0086169` #&gt; $data$path$`10.1371/journal.pone.0086169`$path #&gt; [1] &quot;/Users/sckott/Library/Caches/R/fulltext/10_1371_journal_pone_0086169.xml&quot; #&gt; #&gt; $data$path$`10.1371/journal.pone.0086169`$id #&gt; [1] &quot;10.1371/journal.pone.0086169&quot; #&gt; #&gt; $data$path$`10.1371/journal.pone.0086169`$type #&gt; [1] &quot;xml&quot; #&gt; #&gt; $data$path$`10.1371/journal.pone.0086169`$error #&gt; NULL #&gt; #&gt; #&gt; #&gt; $data$data #&gt; NULL #&gt; #&gt; #&gt; $opts #&gt; $opts$doi #&gt; [1] &quot;10.1371/journal.pone.0086169&quot; #&gt; #&gt; $opts$type #&gt; [1] &quot;xml&quot; You can pass many DOIs in at once (res &lt;- ft_get(c(&#39;10.3389/fphar.2014.00109&#39;, &#39;10.3389/feart.2015.00009&#39;))) #&gt; &lt;fulltext text&gt; #&gt; [Docs] 2 #&gt; [Source] ext - /Users/sckott/Library/Caches/R/fulltext #&gt; [IDs] 10.3389/fphar.2014.00109 10.3389/feart.2015.00009 ... res$frontiersin #&gt; $found #&gt; [1] 2 #&gt; #&gt; $dois #&gt; [1] &quot;10.3389/fphar.2014.00109&quot; &quot;10.3389/feart.2015.00009&quot; #&gt; #&gt; $data #&gt; $data$backend #&gt; [1] &quot;ext&quot; #&gt; #&gt; $data$cache_path #&gt; [1] &quot;/Users/sckott/Library/Caches/R/fulltext&quot; #&gt; #&gt; $data$path #&gt; $data$path$`10.3389/fphar.2014.00109` #&gt; $data$path$`10.3389/fphar.2014.00109`$path #&gt; [1] &quot;/Users/sckott/Library/Caches/R/fulltext/10_3389_fphar_2014_00109.xml&quot; #&gt; #&gt; $data$path$`10.3389/fphar.2014.00109`$id #&gt; [1] &quot;10.3389/fphar.2014.00109&quot; #&gt; #&gt; $data$path$`10.3389/fphar.2014.00109`$type #&gt; [1] &quot;xml&quot; #&gt; #&gt; $data$path$`10.3389/fphar.2014.00109`$error #&gt; NULL #&gt; #&gt; #&gt; $data$path$`10.3389/feart.2015.00009` #&gt; $data$path$`10.3389/feart.2015.00009`$path #&gt; [1] &quot;/Users/sckott/Library/Caches/R/fulltext/10_3389_feart_2015_00009.xml&quot; #&gt; #&gt; $data$path$`10.3389/feart.2015.00009`$id #&gt; [1] &quot;10.3389/feart.2015.00009&quot; #&gt; #&gt; $data$path$`10.3389/feart.2015.00009`$type #&gt; [1] &quot;xml&quot; #&gt; #&gt; $data$path$`10.3389/feart.2015.00009`$error #&gt; NULL #&gt; #&gt; #&gt; #&gt; $data$data #&gt; NULL #&gt; #&gt; #&gt; $opts #&gt; $opts$dois #&gt; [1] &quot;10.3389/fphar.2014.00109&quot; &quot;10.3389/feart.2015.00009&quot; #&gt; #&gt; $opts$type #&gt; [1] &quot;xml&quot; 8.4 Errors A new element of the results of ft_get() is the error slot in each list. If the error slot is NULL then there is no error. If the error slot is not NULL there was an error, and the error message will be a character string in that slot. Possible errors include: An error reported by the web service e.g. “Timeout was reached: Connection timed out after 10003 milliseconds” No link found e.g. “no link found from Crossref” OR “has no link available” We attempted to fetch the article but the content type wasn’t what was expected. In this case we skip to the next article. e.g. “type was supposed to be pdf, but was text/html; charset=UTF-8” Weird uninformative errors e.g. “Recv failure: Operation timed out” OR “Operation was aborted by an application callback” An error associated mostly with PLOS. PLOS gives DOIs for parts of articles, like figures, so it doesn’t make sense to get full text of a figure. e.g., “was not found or may be a DOI for a part of an article” 8.5 Cleanup The above section about errors suggests that we often run into errors. When we run into errors downloading full text we capture the error message, if there is one, and delete the file we were trying to create. That is, we cleanup upon hitting an error such that you shouldn’t end up with blank files on your machine. Let us know if this isn’t true in your case and we’ll get it fixed. Note that even if you exit out of the all to ft_get() it should clean up a file if it is not completey done creating it, so you shouldn’t end up with bad files if you exit out of the function when it’s running. 8.6 Internals What’s going on under the hood in ft_get()? It goes like this: xxxx 8.7 Notes about specific data sources 8.7.1 Elsevier When you don’t have access to the full text of Elsevier articles they will often still give you something, but it will sometimes be just metadata of the paper or sometimes an abstract if you’re lucky. When you go to extract the text this will be rather obvious. "],
["chunks.html", "Chapter 9 Chunks 9.1 Usage 9.2 Tabularize", " Chapter 9 Chunks The ft_chunks function tries to make it easy to extract the parts of articles you want. This only works with XML format articles though since although we can get text out of PDFs, there is no machine readable way to say “I want the abstract”. In addition to only working with XML, this function only has knowledge about a few select publishers for which we’ve coded knowledge about how to get different sections of the article. Not all publishers use the same format XML - so each publisher is slightly different for how to get to each section. That is, to get to the abstract requires slightly different xpath for publisher A vs. publisher B vs. publisher C. 9.1 Usage library(fulltext) Get a full text article x &lt;- ft_get(&#39;10.1371/journal.pone.0086169&#39;, from=&#39;plos&#39;) Note that unlike previous version of fulltext you now have to collect (ft_collect()) the text from the XML file on disk. Then you can pass to ft_chunks(), here to get authors. x %&gt;% ft_collect %&gt;% ft_chunks(what=&quot;authors&quot;) #&gt; $plos #&gt; $plos$`10.1371/journal.pone.0086169` #&gt; $plos$`10.1371/journal.pone.0086169`$authors #&gt; $plos$`10.1371/journal.pone.0086169`$authors[[1]] #&gt; $plos$`10.1371/journal.pone.0086169`$authors[[1]]$given_names #&gt; [1] &quot;Katie&quot; #&gt; #&gt; $plos$`10.1371/journal.pone.0086169`$authors[[1]]$surname #&gt; [1] &quot;Hinde&quot; #&gt; #&gt; #&gt; $plos$`10.1371/journal.pone.0086169`$authors[[2]] #&gt; $plos$`10.1371/journal.pone.0086169`$authors[[2]]$given_names #&gt; [1] &quot;Abigail J.&quot; #&gt; #&gt; $plos$`10.1371/journal.pone.0086169`$authors[[2]]$surname #&gt; [1] &quot;Carpenter&quot; #&gt; #&gt; #&gt; $plos$`10.1371/journal.pone.0086169`$authors[[3]] #&gt; $plos$`10.1371/journal.pone.0086169`$authors[[3]]$given_names #&gt; [1] &quot;John S.&quot; #&gt; #&gt; $plos$`10.1371/journal.pone.0086169`$authors[[3]]$surname #&gt; [1] &quot;Clay&quot; #&gt; #&gt; #&gt; $plos$`10.1371/journal.pone.0086169`$authors[[4]] #&gt; $plos$`10.1371/journal.pone.0086169`$authors[[4]]$given_names #&gt; [1] &quot;Barry J.&quot; #&gt; #&gt; $plos$`10.1371/journal.pone.0086169`$authors[[4]]$surname #&gt; [1] &quot;Bradford&quot; In another example, let’s search for PLOS articles. library(&quot;rplos&quot;) (dois &lt;- searchplos(q=&quot;*:*&quot;, fl=&#39;id&#39;, fq=list(&#39;doc_type:full&#39;,&quot;article_type:\\&quot;research article\\&quot;&quot;), limit=5)$data$id) #&gt; [1] &quot;10.1371/journal.pone.0044136&quot; &quot;10.1371/journal.pone.0155491&quot; #&gt; [3] &quot;10.1371/journal.pone.0058100&quot; &quot;10.1371/journal.pone.0168627&quot; #&gt; [5] &quot;10.1371/journal.pone.0184491&quot; Then get the full text x &lt;- ft_get(dois, from=&quot;plos&quot;) Then pull out various sections of each article. remember to pull out the full text first x &lt;- ft_collect(x) x %&gt;% ft_chunks(&quot;front&quot;) x %&gt;% ft_chunks(&quot;body&quot;) x %&gt;% ft_chunks(&quot;back&quot;) x %&gt;% ft_chunks(&quot;history&quot;) x %&gt;% ft_chunks(&quot;authors&quot;) x %&gt;% ft_chunks(c(&quot;doi&quot;,&quot;categories&quot;)) x %&gt;% ft_chunks(&quot;all&quot;) x %&gt;% ft_chunks(&quot;publisher&quot;) x %&gt;% ft_chunks(&quot;acknowledgments&quot;) x %&gt;% ft_chunks(&quot;permissions&quot;) x %&gt;% ft_chunks(&quot;journal_meta&quot;) x %&gt;% ft_chunks(&quot;article_meta&quot;) 9.2 Tabularize The function ft_tabularize() is useful for coercing the output of ft_chunks() into a data.frame, the lingua franca of data work in R. x %&gt;% ft_chunks(c(&quot;acknowledgments&quot;, &quot;permissions&quot;)) %&gt;% ft_tabularize() #&gt; $plos #&gt; acknowledgments #&gt; 1 The authors thank Dr. Tong Huang and Dr. Jingyi Yuan at Zhenhai Lianhua Hospital, Ningbo, China for their valuable work in data collection. #&gt; 2 This study was funded by the National Natural Science Foundation of China (Grant No. 31301222, QY, XZ, YPH, ZGS), Science and Technology Committee of Shanghai (Grant No. 12JC1407601, YG, JMW, HJS, JW), and the Shanghai Municipal Commission of Health and Family Planning Project of Science and Technology Development Fund (Grant No. 2013JG03, XZ, ZGS). #&gt; 3 &lt;NA&gt; #&gt; 4 The authors would like to thank Dr. Shin-Ru Shih, Dr. Daniel Hsu, Dr. Hsuan-Yu Chen and the members of the Liu lab for fruitful discussions. The authors would like to thank Yu-Jung Chen, Chin-Shan Loo, Ai-Lin Cheng and Tsui-Yen Fan for technical supports. The authors would also like to thank Academia Sinica Translational Medicine Program for the administrative support. #&gt; 5 Derek So had full access to all of the data in the study and takes responsibility for the integrity of the data and the accuracy of the data analysis. We would like to thank ClinicalStudyDataRequest.com for making available the dataset analyzed in this article. #&gt; permissions.copyright.holder #&gt; 1 Xu et al #&gt; 2 Yang et al #&gt; 3 Yang et al #&gt; 4 Huang et al #&gt; 5 So, Knoppers #&gt; permissions.license #&gt; 1 This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited. #&gt; 2 This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited. #&gt; 3 This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited. #&gt; 4 This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited. #&gt; 5 This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited. #&gt; permissions.license_url permissions.copyright.year #&gt; 1 &lt;NA&gt; &lt;NA&gt; #&gt; 2 http://creativecommons.org/licenses/by/4.0/ 2016 #&gt; 3 &lt;NA&gt; 2013 #&gt; 4 http://creativecommons.org/licenses/by/4.0/ 2016 #&gt; 5 http://creativecommons.org/licenses/by/4.0/ 2017 "],
["table.html", "Chapter 10 Table 10.1 Usage", " Chapter 10 Table The ft_table() function makes it easy to create a data.frame of the text of PDF, plain text, and XML files, together with DOIs/IDs for each article. It’s similar to the readtext::readtext() function, but is much more specific to just this package. With the output of ft_table() you can go directly into a text-mining package like quanteda. 10.1 Usage library(fulltext) Use ft_table() to pull out text from all articles. ft_table() #&gt; # A tibble: 192 x 4 #&gt; dois ids_norm text paths #&gt; * &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 10.1002/9783527696109.ch41 10_1002_9783527696109_ch41 &quot; … /Use… #&gt; 2 10.1002/chin.199038056 10_1002_chin_199038056 &quot;ChemInfor… /Use… #&gt; 3 10.1002/cite.330221605 10_1002_cite_330221605 &quot; Versamml… /Use… #&gt; 4 10.1002/dvg.22402 10_1002_dvg_22402 &quot;C 2013 Wi… /Use… #&gt; 5 10.1002/jctb.5010090209 10_1002_jctb_5010090209 &quot; … /Use… #&gt; 6 10.1002/qua.560200801 10_1002_qua_560200801 &quot;Internati… /Use… #&gt; 7 10.1002/risk.200590063 10_1002_risk_200590063 &quot; … /Use… #&gt; 8 10.1002/scin.5591692420 10_1002_scin_5591692420 &quot;Books\\n … /Use… #&gt; 9 10.1006/bbrc.1994.2001 10_1006_bbrc_1994_2001 &quot;http://ap… /Use… #&gt; 10 10.1007/11946465_42 10_1007_11946465_42 &quot; Hoon Cho… /Use… #&gt; # ... with 182 more rows You can pull out just text from XML files ft_table(type = &quot;xml&quot;) #&gt; # A tibble: 105 x 4 #&gt; dois ids_norm text paths #&gt; * &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 10.1006/bbrc.1994.2001 10_1006_bbrc_1994_2001 &quot;http:… /Use… #&gt; 2 10.1016/0002-9378(65)90490-4 10_1016_0002_9378_65_90490_4 &quot;http:… /Use… #&gt; 3 10.1016/0016-7185(95)00044-5 10_1016_0016_7185_95_00044_5 &quot;http:… /Use… #&gt; 4 10.1016/0021-8928(59)90156-x 10_1016_0021_8928_59_90156_x &quot;http:… /Use… #&gt; 5 10.1016/0022-4405(90)90022-y 10_1016_0022_4405_90_90022_y &quot;http:… /Use… #&gt; 6 10.1016/0025-3227(64)90037-4 10_1016_0025_3227_64_90037_4 &quot;http:… /Use… #&gt; 7 10.1016/0039-3681(86)90003-8 10_1016_0039_3681_86_90003_8 &quot;http:… /Use… #&gt; 8 10.1016/0065-2571(81)90024-8 10_1016_0065_2571_81_90024_8 &quot;http:… /Use… #&gt; 9 10.1016/0304-4068(89)90027-x 10_1016_0304_4068_89_90027_x http:/… /Use… #&gt; 10 10.1016/0921-4534(89)90593-5 10_1016_0921_4534_89_90593_5 &quot;http:… /Use… #&gt; # ... with 95 more rows You can pull out just text from PDF files ft_table(type = &quot;pdf&quot;) #&gt; # A tibble: 87 x 4 #&gt; dois ids_norm text paths #&gt; * &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 10.1002/9783527696109.ch41 10_1002_9783527696109_ch41 &quot; … /Use… #&gt; 2 10.1002/chin.199038056 10_1002_chin_199038056 &quot;ChemInfor… /Use… #&gt; 3 10.1002/cite.330221605 10_1002_cite_330221605 &quot; Versamml… /Use… #&gt; 4 10.1002/dvg.22402 10_1002_dvg_22402 &quot;C 2013 Wi… /Use… #&gt; 5 10.1002/jctb.5010090209 10_1002_jctb_5010090209 &quot; … /Use… #&gt; 6 10.1002/qua.560200801 10_1002_qua_560200801 &quot;Internati… /Use… #&gt; 7 10.1002/risk.200590063 10_1002_risk_200590063 &quot; … /Use… #&gt; 8 10.1002/scin.5591692420 10_1002_scin_5591692420 &quot;Books\\n … /Use… #&gt; 9 10.1007/11946465_42 10_1007_11946465_42 &quot; Hoon Cho… /Use… #&gt; 10 10.1007/7171_2009_3 10_1007_7171_2009_3 &quot;eth Neals… /Use… #&gt; # ... with 77 more rows You can pull out XML but not extract the text. So you’ll get XML strings that you can parse yourself with xpath/css selectors/etc. ft_table(xml_extract_text = FALSE) #&gt; # A tibble: 192 x 4 #&gt; dois ids_norm text paths #&gt; * &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 10.1002/9783527696109.ch41 10_1002_9783527696109_ch41 &quot; … /Use… #&gt; 2 10.1002/chin.199038056 10_1002_chin_199038056 &quot;ChemInfor… /Use… #&gt; 3 10.1002/cite.330221605 10_1002_cite_330221605 &quot; Versamml… /Use… #&gt; 4 10.1002/dvg.22402 10_1002_dvg_22402 &quot;C 2013 Wi… /Use… #&gt; 5 10.1002/jctb.5010090209 10_1002_jctb_5010090209 &quot; … /Use… #&gt; 6 10.1002/qua.560200801 10_1002_qua_560200801 &quot;Internati… /Use… #&gt; 7 10.1002/risk.200590063 10_1002_risk_200590063 &quot; … /Use… #&gt; 8 10.1002/scin.5591692420 10_1002_scin_5591692420 &quot;Books\\n … /Use… #&gt; 9 10.1006/bbrc.1994.2001 10_1006_bbrc_1994_2001 &quot;&lt;full-tex… /Use… #&gt; 10 10.1007/11946465_42 10_1007_11946465_42 &quot; Hoon Cho… /Use… #&gt; # ... with 182 more rows "],
["supplementary.html", "Chapter 11 Supplementary", " Chapter 11 Supplementary coming soon … "],
["use-cases.html", "Chapter 12 Use cases", " Chapter 12 Use cases Coming soon … "],
["literature.html", "Chapter 13 Literature", " Chapter 13 Literature Here is a review of existing methods. You can use the ftdoi API from R with the https://github.com/ropenscilabs/rftdoi package.↩ "]
]
